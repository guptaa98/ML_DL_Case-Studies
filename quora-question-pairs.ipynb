{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-09T13:59:22.568016Z","iopub.execute_input":"2021-10-09T13:59:22.568882Z","iopub.status.idle":"2021-10-09T13:59:22.600436Z","shell.execute_reply.started":"2021-10-09T13:59:22.568758Z","shell.execute_reply":"2021-10-09T13:59:22.599580Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.DataFrame()\ntrain = pd.read_csv('../input/quora-question-pairs/train.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:22.602148Z","iopub.execute_input":"2021-10-09T13:59:22.602406Z","iopub.status.idle":"2021-10-09T13:59:25.538485Z","shell.execute_reply.started":"2021-10-09T13:59:22.602375Z","shell.execute_reply":"2021-10-09T13:59:25.537789Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:25.542267Z","iopub.execute_input":"2021-10-09T13:59:25.542577Z","iopub.status.idle":"2021-10-09T13:59:25.563824Z","shell.execute_reply.started":"2021-10-09T13:59:25.542541Z","shell.execute_reply":"2021-10-09T13:59:25.562932Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/quora-question-pairs/test.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:25.566265Z","iopub.execute_input":"2021-10-09T13:59:25.566646Z","iopub.status.idle":"2021-10-09T13:59:46.820164Z","shell.execute_reply.started":"2021-10-09T13:59:25.566612Z","shell.execute_reply":"2021-10-09T13:59:46.819013Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:46.823371Z","iopub.execute_input":"2021-10-09T13:59:46.823792Z","iopub.status.idle":"2021-10-09T13:59:46.836798Z","shell.execute_reply.started":"2021-10-09T13:59:46.823758Z","shell.execute_reply":"2021-10-09T13:59:46.835842Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:46.838209Z","iopub.execute_input":"2021-10-09T13:59:46.838455Z","iopub.status.idle":"2021-10-09T13:59:48.878195Z","shell.execute_reply.started":"2021-10-09T13:59:46.838427Z","shell.execute_reply":"2021-10-09T13:59:48.877228Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:48.879803Z","iopub.execute_input":"2021-10-09T13:59:48.880488Z","iopub.status.idle":"2021-10-09T13:59:48.887652Z","shell.execute_reply.started":"2021-10-09T13:59:48.880435Z","shell.execute_reply":"2021-10-09T13:59:48.886842Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:48.889215Z","iopub.execute_input":"2021-10-09T13:59:48.889675Z","iopub.status.idle":"2021-10-09T13:59:48.898503Z","shell.execute_reply.started":"2021-10-09T13:59:48.889616Z","shell.execute_reply":"2021-10-09T13:59:48.897848Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:48.900508Z","iopub.execute_input":"2021-10-09T13:59:48.900767Z","iopub.status.idle":"2021-10-09T13:59:49.047867Z","shell.execute_reply.started":"2021-10-09T13:59:48.900739Z","shell.execute_reply":"2021-10-09T13:59:49.046997Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train.groupby(\"is_duplicate\")['id'].count().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:49.051770Z","iopub.execute_input":"2021-10-09T13:59:49.052046Z","iopub.status.idle":"2021-10-09T13:59:49.293124Z","shell.execute_reply.started":"2021-10-09T13:59:49.052011Z","shell.execute_reply":"2021-10-09T13:59:49.292053Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(\"total number of questions for training are:- {}\".format(len(train)))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:49.294740Z","iopub.execute_input":"2021-10-09T13:59:49.295007Z","iopub.status.idle":"2021-10-09T13:59:49.300036Z","shell.execute_reply.started":"2021-10-09T13:59:49.294973Z","shell.execute_reply":"2021-10-09T13:59:49.299292Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print('~> Question pairs are not Similar (is_duplicate = 0):\\n   {}%'.format(round(len(train[train['is_duplicate']==0])/len(train) * 100),2))\nprint('\\n~> Question pairs are Similar (is_duplicate = 1):\\n   {}%'.format(round(len(train[train['is_duplicate']==1])/len(train) * 100),2))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:49.301334Z","iopub.execute_input":"2021-10-09T13:59:49.301791Z","iopub.status.idle":"2021-10-09T13:59:49.373064Z","shell.execute_reply.started":"2021-10-09T13:59:49.301758Z","shell.execute_reply":"2021-10-09T13:59:49.372103Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#store all the question ids in a list 'qids'\nqids = pd.Series(train['qid1'].tolist() + train['qid2'].tolist())\n\n#get the unique qids by using np.unique on qids\nunique_qs = len(np.unique(qids))\n\n#get the sum of those qids which repeat more than one\nqs_morethan_onetime = np.sum(qids.value_counts() > 1)\n\nprint ('Total number of  Unique Questions are: {}'.format(unique_qs))\nprint ('Number of unique questions that appear more than one time: {} ({}%)'.format(qs_morethan_onetime,qs_morethan_onetime/unique_qs*100))\nprint ('Max number of times a single question is repeated: {}'.format(max(qids.value_counts()))) ","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:49.374628Z","iopub.execute_input":"2021-10-09T13:59:49.374891Z","iopub.status.idle":"2021-10-09T13:59:50.130258Z","shell.execute_reply.started":"2021-10-09T13:59:49.374861Z","shell.execute_reply":"2021-10-09T13:59:50.129263Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"x = [\"unique_questions\" , \"Repeated Questions\"]\ny = [unique_qs , qs_morethan_onetime]\n\nplt.figure(figsize=(10, 6))\nplt.title (\"Plot representing unique and repeated questions  \")\nsns.barplot(x,y)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:50.131517Z","iopub.execute_input":"2021-10-09T13:59:50.132066Z","iopub.status.idle":"2021-10-09T13:59:50.321141Z","shell.execute_reply.started":"2021-10-09T13:59:50.132033Z","shell.execute_reply":"2021-10-09T13:59:50.320127Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#checking for duplicate pairs by grouping on qid1 and qid2.\nduplicate_pairs = train[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\nprint(\"total number of duplicate questions: {}\".format(duplicate_pairs.shape[0] - train.shape[0])) ","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:50.322343Z","iopub.execute_input":"2021-10-09T13:59:50.322593Z","iopub.status.idle":"2021-10-09T13:59:50.596394Z","shell.execute_reply.started":"2021-10-09T13:59:50.322562Z","shell.execute_reply":"2021-10-09T13:59:50.595327Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#plot occurences of questions\nplt.figure(figsize=(20,10))\nplt.hist(qids.value_counts(),bins=160)\n\nplt.yscale('log', nonposy='clip')\n\nplt.title(\"Occurence of questions\")\nplt.xlabel(\"number of occurence of question\")\nplt.ylabel(\"number of question\")\n\nprint ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) ","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:50.597713Z","iopub.execute_input":"2021-10-09T13:59:50.597975Z","iopub.status.idle":"2021-10-09T13:59:52.119064Z","shell.execute_reply.started":"2021-10-09T13:59:50.597944Z","shell.execute_reply":"2021-10-09T13:59:52.118303Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#check for null values in an entire dataframe\nnull_rows = train[train.isnull().any(axis = 1)]\nnull_rows","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:52.120516Z","iopub.execute_input":"2021-10-09T13:59:52.121406Z","iopub.status.idle":"2021-10-09T13:59:52.255556Z","shell.execute_reply.started":"2021-10-09T13:59:52.121359Z","shell.execute_reply":"2021-10-09T13:59:52.254720Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#fill the nan values with a space\ntrain = train.fillna(\" \")\nnull_rows = train[train.isnull().any(axis = 1)]\nprint(null_rows)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:52.257238Z","iopub.execute_input":"2021-10-09T13:59:52.257558Z","iopub.status.idle":"2021-10-09T13:59:52.568132Z","shell.execute_reply.started":"2021-10-09T13:59:52.257518Z","shell.execute_reply":"2021-10-09T13:59:52.567089Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Basic Feature Extraction before cleaning**\n* **freq_qid1** = Frequency of qid1's\n* freq_qid2 = Frequency of qid2's\n* q1len = Length of q1\n* q2len = Length of q2\n* q1_n_words = Number of words in Question 1\n* q2_n_words = Number of words in Question 2\n* word_Common = (Number of common unique words in Question 1 and Question 2)\n* word_Total =(Total num of words in Question 1 + Total num of words in Question 2)\n* word_share = (word_common)/(word_Total)\n* freq_q1+freq_q2 = sum total of frequency of qid1 and qid2\n* freq_q1-freq_q2 = absolute difference of frequency of qid1 and qid2","metadata":{}},{"cell_type":"code","source":"train['freq_qid1'] = train.groupby('qid1')['qid1'].transform('count') \ntrain['freq_qid2'] = train.groupby('qid2')['qid2'].transform('count')\n\ntrain['q1len'] = train['question1'].astype(str).str.len()\ntrain['q2len'] = train['question2'].astype(str).str.len()\n\ntrain['q1_n_words'] = train['question1'].astype(str).apply(lambda row : len(row.split(\" \")))\ntrain['q2_n_words'] = train['question2'].astype(str).apply(lambda row : len(row.split(\" \")))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:52.569508Z","iopub.execute_input":"2021-10-09T13:59:52.569768Z","iopub.status.idle":"2021-10-09T13:59:54.624545Z","shell.execute_reply.started":"2021-10-09T13:59:52.569739Z","shell.execute_reply":"2021-10-09T13:59:54.623655Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:54.625896Z","iopub.execute_input":"2021-10-09T13:59:54.626137Z","iopub.status.idle":"2021-10-09T13:59:54.641917Z","shell.execute_reply.started":"2021-10-09T13:59:54.626109Z","shell.execute_reply":"2021-10-09T13:59:54.640889Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train.values","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:54.643183Z","iopub.execute_input":"2021-10-09T13:59:54.644226Z","iopub.status.idle":"2021-10-09T13:59:54.922615Z","shell.execute_reply.started":"2021-10-09T13:59:54.644165Z","shell.execute_reply":"2021-10-09T13:59:54.921786Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train['word_Overlap'] = [set(x[3].split()) & set(x[4].split()) for x in train.values]\ntrain['word_Common'] = train['word_Overlap'].str.len()\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:54.923808Z","iopub.execute_input":"2021-10-09T13:59:54.924071Z","iopub.status.idle":"2021-10-09T13:59:59.523870Z","shell.execute_reply.started":"2021-10-09T13:59:54.924041Z","shell.execute_reply":"2021-10-09T13:59:59.523101Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train['word_Total'] = train['question1'].str.split().map(len) + train['question2'].str.split().map(len)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T13:59:59.525298Z","iopub.execute_input":"2021-10-09T13:59:59.525534Z","iopub.status.idle":"2021-10-09T14:00:04.155448Z","shell.execute_reply.started":"2021-10-09T13:59:59.525506Z","shell.execute_reply":"2021-10-09T14:00:04.154632Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.156896Z","iopub.execute_input":"2021-10-09T14:00:04.157138Z","iopub.status.idle":"2021-10-09T14:00:04.175078Z","shell.execute_reply.started":"2021-10-09T14:00:04.157111Z","shell.execute_reply":"2021-10-09T14:00:04.174083Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train.drop('word_Overlap' , axis = 1, inplace = True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.176505Z","iopub.execute_input":"2021-10-09T14:00:04.176758Z","iopub.status.idle":"2021-10-09T14:00:04.362535Z","shell.execute_reply.started":"2021-10-09T14:00:04.176732Z","shell.execute_reply":"2021-10-09T14:00:04.361536Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train['word_share'] = train['word_Common'] / train['word_Total'] ","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.364021Z","iopub.execute_input":"2021-10-09T14:00:04.364258Z","iopub.status.idle":"2021-10-09T14:00:04.372832Z","shell.execute_reply.started":"2021-10-09T14:00:04.364230Z","shell.execute_reply":"2021-10-09T14:00:04.371828Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.374443Z","iopub.execute_input":"2021-10-09T14:00:04.374727Z","iopub.status.idle":"2021-10-09T14:00:04.394166Z","shell.execute_reply.started":"2021-10-09T14:00:04.374678Z","shell.execute_reply":"2021-10-09T14:00:04.393111Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train['freq_q1+q2'] = train['freq_qid1'] + train['freq_qid2']\ntrain['freq_q1-q2'] = abs(train['freq_qid1'] - train['freq_qid2'])","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.399959Z","iopub.execute_input":"2021-10-09T14:00:04.400655Z","iopub.status.idle":"2021-10-09T14:00:04.416191Z","shell.execute_reply.started":"2021-10-09T14:00:04.400611Z","shell.execute_reply":"2021-10-09T14:00:04.415402Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.417510Z","iopub.execute_input":"2021-10-09T14:00:04.417786Z","iopub.status.idle":"2021-10-09T14:00:04.434957Z","shell.execute_reply.started":"2021-10-09T14:00:04.417757Z","shell.execute_reply":"2021-10-09T14:00:04.434015Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#Analysis of extracted features\nprint (\"Minimum length of the questions in question1 : \" , min(train['q1_n_words']))\nprint (\"Minimum length of the questions in question2 : \" , min(train['q2_n_words']))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.436766Z","iopub.execute_input":"2021-10-09T14:00:04.437286Z","iopub.status.idle":"2021-10-09T14:00:04.563457Z","shell.execute_reply.started":"2021-10-09T14:00:04.437239Z","shell.execute_reply":"2021-10-09T14:00:04.562644Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print (\"Number of Questions with minimum length [question1] :\", train[train['q1_n_words']== 1].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", train[train['q2_n_words']== 1].shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.564624Z","iopub.execute_input":"2021-10-09T14:00:04.564898Z","iopub.status.idle":"2021-10-09T14:00:04.622892Z","shell.execute_reply.started":"2021-10-09T14:00:04.564868Z","shell.execute_reply":"2021-10-09T14:00:04.621845Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = train[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(train[train['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = 'red')\nsns.distplot(train[train['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:04.624327Z","iopub.execute_input":"2021-10-09T14:00:04.624576Z","iopub.status.idle":"2021-10-09T14:00:08.632086Z","shell.execute_reply.started":"2021-10-09T14:00:04.624548Z","shell.execute_reply":"2021-10-09T14:00:08.631254Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_Common', data = train[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(train[train['is_duplicate'] == 1.0]['word_Common'][0:] , label = \"1\", color = 'red')\nsns.distplot(train[train['is_duplicate'] == 0.0]['word_Common'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:08.633457Z","iopub.execute_input":"2021-10-09T14:00:08.634402Z","iopub.status.idle":"2021-10-09T14:00:12.234196Z","shell.execute_reply.started":"2021-10-09T14:00:08.634354Z","shell.execute_reply":"2021-10-09T14:00:12.233261Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:12.235564Z","iopub.execute_input":"2021-10-09T14:00:12.235840Z","iopub.status.idle":"2021-10-09T14:00:12.251991Z","shell.execute_reply.started":"2021-10-09T14:00:12.235812Z","shell.execute_reply":"2021-10-09T14:00:12.251213Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing of Text\n* Removing HTML tags\n* Remove punctuation\n* Stemming\n* Removing stop words\n* Expanding contractions","metadata":{}},{"cell_type":"code","source":"STOP_WORDS = stopwords.words(\"english\")\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n               \n    \n    return x","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:12.253392Z","iopub.execute_input":"2021-10-09T14:00:12.253647Z","iopub.status.idle":"2021-10-09T14:00:12.271926Z","shell.execute_reply.started":"2021-10-09T14:00:12.253619Z","shell.execute_reply":"2021-10-09T14:00:12.270952Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"<h2> 3.5 Advanced Feature Extraction (NLP and Fuzzy Features) </h2>","metadata":{}},{"cell_type":"code","source":"SAFE_DIV = 0.0001","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:12.273600Z","iopub.execute_input":"2021-10-09T14:00:12.274187Z","iopub.status.idle":"2021-10-09T14:00:12.278556Z","shell.execute_reply.started":"2021-10-09T14:00:12.274140Z","shell.execute_reply":"2021-10-09T14:00:12.277613Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"!pip install --user distance","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:12.279848Z","iopub.execute_input":"2021-10-09T14:00:12.280091Z","iopub.status.idle":"2021-10-09T14:00:24.392095Z","shell.execute_reply.started":"2021-10-09T14:00:12.280064Z","shell.execute_reply":"2021-10-09T14:00:24.391095Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def get_token_features(q1, q2):\n    \n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    \n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)    #cwc_min\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)    #cwc_max\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)    #csc_min\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)    #csc_max\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV) #ctc_min\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV) #ctc_max\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])  #last_word_eq\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])    #first_word_eq\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))  #abs_len_diff\n    \n    #Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2   #mean_len\n    return token_features\n\n# get the Longest Common sub string\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    \n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n   \n    #Computing Fuzzy Features and Merging with Dataset\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:24.396937Z","iopub.execute_input":"2021-10-09T14:00:24.397241Z","iopub.status.idle":"2021-10-09T14:00:24.426889Z","shell.execute_reply.started":"2021-10-09T14:00:24.397207Z","shell.execute_reply":"2021-10-09T14:00:24.425869Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n# This package is used for finding longest common subsequence between two strings\n# you can write your own dp code for this\nimport distance\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom sklearn.manifold import TSNE\n# Import the Required lib packages for WORD-Cloud generation\n# https://stackoverflow.com/questions/45625434/how-to-install-wordcloud-in-python3-6\nfrom wordcloud import WordCloud, STOPWORDS\nfrom os import path\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:24.427928Z","iopub.execute_input":"2021-10-09T14:00:24.428262Z","iopub.status.idle":"2021-10-09T14:00:24.596937Z","shell.execute_reply.started":"2021-10-09T14:00:24.428229Z","shell.execute_reply":"2021-10-09T14:00:24.595995Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train = extract_features(train)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:00:24.598760Z","iopub.execute_input":"2021-10-09T14:00:24.599042Z","iopub.status.idle":"2021-10-09T14:17:43.974202Z","shell.execute_reply.started":"2021-10-09T14:00:24.599012Z","shell.execute_reply":"2021-10-09T14:17:43.973096Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:17:43.975374Z","iopub.execute_input":"2021-10-09T14:17:43.975625Z","iopub.status.idle":"2021-10-09T14:17:43.998592Z","shell.execute_reply.started":"2021-10-09T14:17:43.975576Z","shell.execute_reply":"2021-10-09T14:17:43.997333Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Analysis of Extracted Features","metadata":{}},{"cell_type":"code","source":"train_duplicate = train[train['is_duplicate'] == 1]\ntrain_nonduplicate = train[train['is_duplicate'] == 0]","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:17:43.999960Z","iopub.execute_input":"2021-10-09T14:17:44.000196Z","iopub.status.idle":"2021-10-09T14:17:44.132416Z","shell.execute_reply.started":"2021-10-09T14:17:44.000169Z","shell.execute_reply":"2021-10-09T14:17:44.131091Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([train_duplicate[\"question1\"], train_duplicate[\"question2\"]]).flatten()\nn = np.dstack([train_nonduplicate[\"question1\"], train_nonduplicate[\"question2\"]]).flatten()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:17:44.133875Z","iopub.execute_input":"2021-10-09T14:17:44.134140Z","iopub.status.idle":"2021-10-09T14:17:44.207345Z","shell.execute_reply.started":"2021-10-09T14:17:44.134109Z","shell.execute_reply":"2021-10-09T14:17:44.206138Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:17:44.208933Z","iopub.execute_input":"2021-10-09T14:17:44.209182Z","iopub.status.idle":"2021-10-09T14:17:44.214066Z","shell.execute_reply.started":"2021-10-09T14:17:44.209155Z","shell.execute_reply":"2021-10-09T14:17:44.213502Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#Saving the np array into a text file\nnp.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:17:44.215018Z","iopub.execute_input":"2021-10-09T14:17:44.215207Z","iopub.status.idle":"2021-10-09T14:17:46.079992Z","shell.execute_reply.started":"2021-10-09T14:17:44.215180Z","shell.execute_reply":"2021-10-09T14:17:46.078977Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# reading the text files and removing the Stop Words:\n#d = path.dirname('.')\n\ntextp_w = open('train_p.txt').read()\ntextn_w = open('train_n.txt').read()\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\n#stopwords.remove(\"good\")\n#stopwords.remove(\"love\")\nstopwords.remove(\"like\")\n#stopwords.remove(\"best\")\n#stopwords.remove(\"!\")\nprint (\"Total number of words in duplicate pair questions :\",len(textp_w))\nprint (\"Total number of words in non duplicate pair questions :\",len(textn_w))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:17:46.081555Z","iopub.execute_input":"2021-10-09T14:17:46.081949Z","iopub.status.idle":"2021-10-09T14:17:46.405751Z","shell.execute_reply.started":"2021-10-09T14:17:46.081910Z","shell.execute_reply":"2021-10-09T14:17:46.404892Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:17:46.407413Z","iopub.execute_input":"2021-10-09T14:17:46.407770Z","iopub.status.idle":"2021-10-09T14:17:55.886019Z","shell.execute_reply.started":"2021-10-09T14:17:46.407727Z","shell.execute_reply":"2021-10-09T14:17:55.884861Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textn_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Non Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:17:55.887474Z","iopub.execute_input":"2021-10-09T14:17:55.887748Z","iopub.status.idle":"2021-10-09T14:18:05.496112Z","shell.execute_reply.started":"2021-10-09T14:17:55.887708Z","shell.execute_reply":"2021-10-09T14:18:05.495490Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"n = train.shape[0]\nsns.pairplot(train[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:18:05.497379Z","iopub.execute_input":"2021-10-09T14:18:05.497786Z","iopub.status.idle":"2021-10-09T14:22:46.637580Z","shell.execute_reply.started":"2021-10-09T14:18:05.497755Z","shell.execute_reply":"2021-10-09T14:22:46.636882Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"train.is_duplicate.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:22:46.638700Z","iopub.execute_input":"2021-10-09T14:22:46.639401Z","iopub.status.idle":"2021-10-09T14:22:46.649541Z","shell.execute_reply.started":"2021-10-09T14:22:46.639362Z","shell.execute_reply":"2021-10-09T14:22:46.648702Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = train[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(train[train['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(train[train['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:22:46.650833Z","iopub.execute_input":"2021-10-09T14:22:46.651178Z","iopub.status.idle":"2021-10-09T14:22:50.665680Z","shell.execute_reply.started":"2021-10-09T14:22:46.651143Z","shell.execute_reply":"2021-10-09T14:22:50.664543Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = train[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(train[train['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(train[train['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:22:50.667260Z","iopub.execute_input":"2021-10-09T14:22:50.667528Z","iopub.status.idle":"2021-10-09T14:22:54.763968Z","shell.execute_reply.started":"2021-10-09T14:22:50.667497Z","shell.execute_reply":"2021-10-09T14:22:54.762804Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:22:54.766158Z","iopub.execute_input":"2021-10-09T14:22:54.766489Z","iopub.status.idle":"2021-10-09T14:22:54.772966Z","shell.execute_reply.started":"2021-10-09T14:22:54.766446Z","shell.execute_reply":"2021-10-09T14:22:54.772249Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"train.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:22:54.774088Z","iopub.execute_input":"2021-10-09T14:22:54.774330Z","iopub.status.idle":"2021-10-09T14:22:54.804616Z","shell.execute_reply.started":"2021-10-09T14:22:54.774302Z","shell.execute_reply":"2021-10-09T14:22:54.803759Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"pip install spacy","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:22:54.806120Z","iopub.execute_input":"2021-10-09T14:22:54.807063Z","iopub.status.idle":"2021-10-09T14:23:03.096293Z","shell.execute_reply.started":"2021-10-09T14:22:54.807024Z","shell.execute_reply":"2021-10-09T14:23:03.095015Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"questions = train['question1'] + train['question2']","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:23:03.098272Z","iopub.execute_input":"2021-10-09T14:23:03.098556Z","iopub.status.idle":"2021-10-09T14:23:03.228484Z","shell.execute_reply.started":"2021-10-09T14:23:03.098522Z","shell.execute_reply":"2021-10-09T14:23:03.227557Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"i = 0\nlist_of_sentence = []\nfor sentence in questions:\n    list_of_sentence.append(sentence)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:23:03.230080Z","iopub.execute_input":"2021-10-09T14:23:03.230339Z","iopub.status.idle":"2021-10-09T14:23:03.366441Z","shell.execute_reply.started":"2021-10-09T14:23:03.230308Z","shell.execute_reply":"2021-10-09T14:23:03.365799Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import nltk","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:23:03.367466Z","iopub.execute_input":"2021-10-09T14:23:03.368215Z","iopub.status.idle":"2021-10-09T14:23:03.372165Z","shell.execute_reply.started":"2021-10-09T14:23:03.368184Z","shell.execute_reply":"2021-10-09T14:23:03.371206Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"list_of_sentence = [nltk.word_tokenize(sent) for sent in list_of_sentence]","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:23:03.373198Z","iopub.execute_input":"2021-10-09T14:23:03.373432Z","iopub.status.idle":"2021-10-09T14:24:48.519399Z","shell.execute_reply.started":"2021-10-09T14:23:03.373407Z","shell.execute_reply":"2021-10-09T14:24:48.518482Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"import gensim\n#from gensim.models import Word2Vec\nw2vmodel = gensim.models.Word2Vec(list_of_sentence,min_count = 5, vector_size = 50, workers = 4)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:24:48.520882Z","iopub.execute_input":"2021-10-09T14:24:48.521110Z","iopub.status.idle":"2021-10-09T14:25:34.466403Z","shell.execute_reply.started":"2021-10-09T14:24:48.521084Z","shell.execute_reply":"2021-10-09T14:25:34.465808Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"print(w2vmodel.wv.most_similar('sun'))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:25:34.467248Z","iopub.execute_input":"2021-10-09T14:25:34.467458Z","iopub.status.idle":"2021-10-09T14:25:34.483565Z","shell.execute_reply.started":"2021-10-09T14:25:34.467433Z","shell.execute_reply":"2021-10-09T14:25:34.482646Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"print(w2vmodel.wv.most_similar('best'))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:25:34.488654Z","iopub.execute_input":"2021-10-09T14:25:34.492095Z","iopub.status.idle":"2021-10-09T14:25:34.505593Z","shell.execute_reply.started":"2021-10-09T14:25:34.492037Z","shell.execute_reply":"2021-10-09T14:25:34.504389Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"w2v_words = list(w2vmodel.wv.key_to_index)\nprint(\"number of words that occured minimum 5 times \",len(w2v_words))\nprint(\"sample words \", w2v_words[0:50])","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:25:34.511570Z","iopub.execute_input":"2021-10-09T14:25:34.514648Z","iopub.status.idle":"2021-10-09T14:25:34.533255Z","shell.execute_reply.started":"2021-10-09T14:25:34.514591Z","shell.execute_reply":"2021-10-09T14:25:34.532335Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"i = 0\nlist_of_sentences = []\nfor sentence in questions:\n    list_of_sentences.append(sentence)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:25:34.538819Z","iopub.execute_input":"2021-10-09T14:25:34.543214Z","iopub.status.idle":"2021-10-09T14:25:34.718759Z","shell.execute_reply.started":"2021-10-09T14:25:34.543118Z","shell.execute_reply":"2021-10-09T14:25:34.718016Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,1), min_df=10)\ntf_idf_vect.fit(list_of_sentences)\nprint(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf_vect.transform(list_of_sentences)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:25:34.720956Z","iopub.execute_input":"2021-10-09T14:25:34.721295Z","iopub.status.idle":"2021-10-09T14:25:58.956085Z","shell.execute_reply.started":"2021-10-09T14:25:34.721254Z","shell.execute_reply":"2021-10-09T14:25:58.955055Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"dictionary = dict(zip(tf_idf_vect.get_feature_names(), list(tf_idf_vect.idf_)))","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:25:58.959535Z","iopub.execute_input":"2021-10-09T14:25:58.959798Z","iopub.status.idle":"2021-10-09T14:25:58.988552Z","shell.execute_reply.started":"2021-10-09T14:25:58.959771Z","shell.execute_reply":"2021-10-09T14:25:58.987388Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"dictionary['step']","metadata":{"execution":{"iopub.status.busy":"2021-10-09T14:25:58.989983Z","iopub.execute_input":"2021-10-09T14:25:58.990249Z","iopub.status.idle":"2021-10-09T14:25:58.996873Z","shell.execute_reply.started":"2021-10-09T14:25:58.990216Z","shell.execute_reply":"2021-10-09T14:25:58.996097Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"q1_feat = [nltk.word_tokenize(sent) for sent in train['question1']]\nq2_feat = [nltk.word_tokenize(sent) for sent in train['question2']]","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:38:54.611328Z","iopub.execute_input":"2021-10-09T15:38:54.612013Z","iopub.status.idle":"2021-10-09T15:41:14.941618Z","shell.execute_reply.started":"2021-10-09T15:38:54.611977Z","shell.execute_reply":"2021-10-09T15:41:14.940747Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm  \n# TF-IDF weighted Word2Vec\ntfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors_q1 = []; # the tfidf-w2v for each sentence/review is stored in this list\nrow=0;\nfor sent in tqdm.tqdm(q1_feat): # for each review/sentence \n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence/review\n    #print(sent)\n    for word in sent: # for each word in a review/sentence\n        #print('each word')\n        #print(word)\n        if word in w2v_words and word in tfidf_feat:\n            vec = w2vmodel.wv[word]\n            #print('vector')\n            #print(vec)\n#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n            # to reduce the computation we are \n            # dictionary[word] = idf value of word in whole courpus\n            # sent.count(word) = tf valeus of word in this review\n            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec /= weight_sum\n    tfidf_sent_vectors_q1.append(sent_vec)\n    row += 1","metadata":{"execution":{"iopub.status.busy":"2021-10-09T15:43:34.717515Z","iopub.execute_input":"2021-10-09T15:43:34.718640Z","iopub.status.idle":"2021-10-09T16:09:36.809152Z","shell.execute_reply.started":"2021-10-09T15:43:34.718588Z","shell.execute_reply":"2021-10-09T16:09:36.807021Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"tfidf_sent_vectors_q1[0:2]","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:10:01.588992Z","iopub.execute_input":"2021-10-09T16:10:01.589311Z","iopub.status.idle":"2021-10-09T16:10:01.601124Z","shell.execute_reply.started":"2021-10-09T16:10:01.589280Z","shell.execute_reply":"2021-10-09T16:10:01.600107Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"import tqdm  \n# TF-IDF weighted Word2Vec\ntfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors_q2 = []; # the tfidf-w2v for each sentence/review is stored in this list\nrow=0;\nfor sent in tqdm.tqdm(q2_feat): # for each review/sentence \n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence/review\n    #print(sent)\n    for word in sent: # for each word in a review/sentence\n        #print('each word')\n        #print(word)\n        if word in w2v_words and word in tfidf_feat:\n            vec = w2vmodel.wv[word]\n            #print('vector')\n            #print(vec)\n#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n            # to reduce the computation we are \n            # dictionary[word] = idf value of word in whole courpus\n            # sent.count(word) = tf valeus of word in this review\n            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec /= weight_sum\n    tfidf_sent_vectors_q2.append(sent_vec)\n    row += 1","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:11:02.786070Z","iopub.execute_input":"2021-10-09T16:11:02.786611Z","iopub.status.idle":"2021-10-09T16:36:54.474275Z","shell.execute_reply.started":"2021-10-09T16:11:02.786560Z","shell.execute_reply":"2021-10-09T16:36:54.473260Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"tfidf_sent_vectors_q2[0:2]","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:39:41.323574Z","iopub.execute_input":"2021-10-09T16:39:41.323943Z","iopub.status.idle":"2021-10-09T16:39:41.334187Z","shell.execute_reply.started":"2021-10-09T16:39:41.323910Z","shell.execute_reply":"2021-10-09T16:39:41.333006Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['q1_feat'] = list(tfidf_sent_vectors_q1)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:43:15.748527Z","iopub.execute_input":"2021-10-09T16:43:15.748884Z","iopub.status.idle":"2021-10-09T16:43:16.055708Z","shell.execute_reply.started":"2021-10-09T16:43:15.748850Z","shell.execute_reply":"2021-10-09T16:43:16.054731Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:43:20.102427Z","iopub.execute_input":"2021-10-09T16:43:20.102775Z","iopub.status.idle":"2021-10-09T16:43:20.122603Z","shell.execute_reply.started":"2021-10-09T16:43:20.102746Z","shell.execute_reply":"2021-10-09T16:43:20.121288Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"df['q2_feat'] = tfidf_sent_vectors_q2\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:44:50.059714Z","iopub.execute_input":"2021-10-09T16:44:50.060079Z","iopub.status.idle":"2021-10-09T16:44:50.140969Z","shell.execute_reply.started":"2021-10-09T16:44:50.060046Z","shell.execute_reply":"2021-10-09T16:44:50.139928Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"df3_q1 = pd.DataFrame(df.q1_feat.values.tolist(), index= df.index)\ndf3_q2 = pd.DataFrame(df.q2_feat.values.tolist(), index= df.index)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:45:27.946673Z","iopub.execute_input":"2021-10-09T16:45:27.947551Z","iopub.status.idle":"2021-10-09T16:46:01.264353Z","shell.execute_reply.started":"2021-10-09T16:45:27.947508Z","shell.execute_reply":"2021-10-09T16:46:01.263333Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"df3_q1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:49:16.964758Z","iopub.execute_input":"2021-10-09T16:49:16.966042Z","iopub.status.idle":"2021-10-09T16:49:16.991055Z","shell.execute_reply.started":"2021-10-09T16:49:16.965981Z","shell.execute_reply":"2021-10-09T16:49:16.990494Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:55:14.199314Z","iopub.execute_input":"2021-10-09T16:55:14.199613Z","iopub.status.idle":"2021-10-09T16:55:14.227879Z","shell.execute_reply.started":"2021-10-09T16:55:14.199585Z","shell.execute_reply":"2021-10-09T16:55:14.226862Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"df3_q2.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:01:44.242638Z","iopub.execute_input":"2021-10-09T17:01:44.243461Z","iopub.status.idle":"2021-10-09T17:01:44.272150Z","shell.execute_reply.started":"2021-10-09T17:01:44.243425Z","shell.execute_reply":"2021-10-09T17:01:44.271314Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:11:04.013534Z","iopub.execute_input":"2021-10-09T17:11:04.014142Z","iopub.status.idle":"2021-10-09T17:11:04.023713Z","shell.execute_reply.started":"2021-10-09T17:11:04.014091Z","shell.execute_reply":"2021-10-09T17:11:04.022231Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"train.drop(['qid1', 'qid2', 'question1', 'question2'],inplace = True, axis = 1)\n#train.head","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:15:14.563079Z","iopub.execute_input":"2021-10-09T17:15:14.563463Z","iopub.status.idle":"2021-10-09T17:15:14.795440Z","shell.execute_reply.started":"2021-10-09T17:15:14.563418Z","shell.execute_reply":"2021-10-09T17:15:14.794621Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:15:24.175240Z","iopub.execute_input":"2021-10-09T17:15:24.175623Z","iopub.status.idle":"2021-10-09T17:15:24.201253Z","shell.execute_reply.started":"2021-10-09T17:15:24.175560Z","shell.execute_reply":"2021-10-09T17:15:24.200181Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"df3_q1['id']= train['id']\ndf3_q2['id']= train['id']\ntrain  = train.merge(df3_q1, on='id',how='left')\ntrain = train.merge(df3_q2, on='id',how='left')\n#result  = df1.merge(df2, on='id',how='left')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:18:29.334483Z","iopub.execute_input":"2021-10-09T17:18:29.335639Z","iopub.status.idle":"2021-10-09T17:18:30.790131Z","shell.execute_reply.started":"2021-10-09T17:18:29.335582Z","shell.execute_reply":"2021-10-09T17:18:30.789178Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:18:43.455038Z","iopub.execute_input":"2021-10-09T17:18:43.455321Z","iopub.status.idle":"2021-10-09T17:18:43.478588Z","shell.execute_reply.started":"2021-10-09T17:18:43.455291Z","shell.execute_reply":"2021-10-09T17:18:43.477977Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"train.drop('is_duplicate', inplace= True, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:21:51.514391Z","iopub.execute_input":"2021-10-09T17:21:51.515680Z","iopub.status.idle":"2021-10-09T17:21:52.271213Z","shell.execute_reply.started":"2021-10-09T17:21:51.515588Z","shell.execute_reply":"2021-10-09T17:21:52.270375Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"train_new = pd.DataFrame()\ntrain_new = pd.read_csv('../input/quora-question-pairs/train.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:23:19.241437Z","iopub.execute_input":"2021-10-09T17:23:19.242197Z","iopub.status.idle":"2021-10-09T17:23:21.995013Z","shell.execute_reply.started":"2021-10-09T17:23:19.242153Z","shell.execute_reply":"2021-10-09T17:23:21.994388Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"y = train_new['is_duplicate']","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:25:07.883188Z","iopub.execute_input":"2021-10-09T17:25:07.883520Z","iopub.status.idle":"2021-10-09T17:25:07.890202Z","shell.execute_reply.started":"2021-10-09T17:25:07.883487Z","shell.execute_reply":"2021-10-09T17:25:07.888963Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:25:28.272022Z","iopub.execute_input":"2021-10-09T17:25:28.273118Z","iopub.status.idle":"2021-10-09T17:25:28.285307Z","shell.execute_reply.started":"2021-10-09T17:25:28.273074Z","shell.execute_reply":"2021-10-09T17:25:28.284318Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}